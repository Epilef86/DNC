{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyPZXTYgdDFlU00krcI1BGbv",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Epilef86/DNC/blob/main/Spark_SQL.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "A vantagem de se utilizar SQL no spark é a paralelização dos dados, o spark se tiver um banco com 1 bilhão de linhas ele vai paralelizar esses dados dentro do cluster e dentro da memória de uma meneira muito mais rápida."
      ],
      "metadata": {
        "id": "k1Ew_ekZK6zy"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install pyspark"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "sw9ADMljLB4Q",
        "outputId": "7a2549c7-4379-4845-d03f-e9ef826e1326"
      },
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Requirement already satisfied: pyspark in /usr/local/lib/python3.8/dist-packages (3.3.1)\n",
            "Requirement already satisfied: py4j==0.10.9.5 in /usr/local/lib/python3.8/dist-packages (from pyspark) (0.10.9.5)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!wget -qnc https://bin.equinox.io/c/4VmOzA7iaHb/ngrok-stable-linux-amd64.zip\n",
        "!unzip -n -q ngrok-stable-linux-amd64.zip"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "6tUwMTJ8LB0_",
        "outputId": "dbd863dc-1aee-4f48-b036-80bf84c6825d"
      },
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "unzip:  cannot find or open ngrok-stable-linux-amd64.zip, ngrok-stable-linux-amd64.zip.zip or ngrok-stable-linux-amd64.zip.ZIP.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 16,
      "metadata": {
        "id": "4fL-N7e1KFk3"
      },
      "outputs": [],
      "source": [
        "from pyspark.sql import SparkSession\n",
        "\n",
        "spark =(\n",
        "    SparkSession.builder\n",
        "    .config('spark.ui.port', '4050')\n",
        "    .appName('SparkSQL')\n",
        "    .getOrCreate()\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!./ngrok authtoken 2KBeQEmmd1YNlQ86GGKf3Kf3KFOkb3_6sQH7JEnVEhDxwn9A7WnT\n",
        "get_ipython().system_raw('./ngrok http 4050 &')\n",
        "!sleep 10\n",
        "!curl -s http://localhost:4040/api/tunnels | grep -Po 'public_url':'(?=https)\\K[^']*'"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "TgF5sG9jMT3Q",
        "outputId": "98d4de3f-3f8c-4938-8a30-a440b7b315c0"
      },
      "execution_count": 17,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "/bin/bash: ./ngrok: No such file or directory\n",
            "/bin/bash: -c: line 0: unexpected EOF while looking for matching `''\n",
            "/bin/bash: -c: line 1: syntax error: unexpected end of file\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from pyspark.sql.types import StructType, StructField, IntegerType, DoubleType, StringType, TimestampNTZType "
      ],
      "metadata": {
        "id": "V8e0HwrVNpmr"
      },
      "execution_count": 18,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "schema_remetente_destinatario = StructType([\n",
        "    StructField('nome', StringType()),\n",
        "    StructField('banco', StringType()),\n",
        "    StructField('tipo', StringType()),\n",
        "])\n",
        "\n",
        "schema_base_pix = StructType([\n",
        "    StructField('id_transacao', IntegerType()),\n",
        "    StructField('valor', DoubleType()),\n",
        "    StructField('remetente ', schema_remetente_destinatario),\n",
        "    StructField('destinatario', schema_remetente_destinatario),\n",
        "    StructField('transaction_date', TimestampType()),\n",
        "    StructField('chave_pix', StringType()),\n",
        "    StructField('fraude', IntegerType())\n",
        "])\n",
        "\n",
        "caminho_json = './pix_transactions.json'\n",
        "\n",
        "df = spark.read.json(\n",
        "    caminho_json,\n",
        "    schema = schema_base_pix,\n",
        "    timestampFormat = \"yyyy-MM-dd\"\n",
        ")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 235
        },
        "id": "BfzKrB3POeQl",
        "outputId": "6c93a96a-2e9c-4c29-b60d-59749058ed06"
      },
      "execution_count": 19,
      "outputs": [
        {
          "output_type": "error",
          "ename": "NameError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-19-d7f90a0e793b>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     10\u001b[0m     \u001b[0mStructField\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'remetente '\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mschema_remetente_destinatario\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     11\u001b[0m     \u001b[0mStructField\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'destinatario'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mschema_remetente_destinatario\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 12\u001b[0;31m     \u001b[0mStructField\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'transaction_date'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mTimestampType\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     13\u001b[0m     \u001b[0mStructField\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'chave_pix'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mStringType\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     14\u001b[0m     \u001b[0mStructField\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'fraude'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mIntegerType\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mNameError\u001b[0m: name 'TimestampType' is not defined"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Como transformar isso em uma tabela pra que eu execute através do SQL, crio uma view temporária, ou seja uma tabela com nome: transacoes_pix, ao invés de um dataframe"
      ],
      "metadata": {
        "id": "kGz3dBRMO4g8"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "spark.read.json(\n",
        "    caminho_json,\n",
        "    schema = schema_base_pix,\n",
        "    timestampFormat = \"yyyy-MM-dd\"\n",
        ").createOrReplaceTempView('transacoes_pix')"
      ],
      "metadata": {
        "id": "FkRM9974PQO4"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Pra executar uma SQL dentro de uma sessão spark , basta chamar a sessão spark"
      ],
      "metadata": {
        "id": "Zd91JShIPpaF"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "spark.sql('select * from transacoes_pix limit 10').show()"
      ],
      "metadata": {
        "id": "NCkMRiaJPdWR"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Retornou dataframe como gostaria"
      ],
      "metadata": {
        "id": "j3IN7vpdP-cd"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Como saber se é mais rápido pelo sql ou executar atraves do dataframe. Vamos fazer um teste. Primeiro fazer uma grupamento por chave pix, contar o numero de transacoes por chave pix"
      ],
      "metadata": {
        "id": "NSImloMJQw2G"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "group_sql = spark.sql('select chave_pix, count(*) from transacoes_pix group by chave_pix)"
      ],
      "metadata": {
        "id": "XHo17gNGRANs"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Segunda maneira de executar em formto de api "
      ],
      "metadata": {
        "id": "JwgpfYJYRcKc"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "group_df = df.groupBy('chave_pix').count(  )"
      ],
      "metadata": {
        "id": "V3bFPY5MRhZb"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Quando der um show nos dois vai ser executado a ação, mas antes disso podemos verificar qual vai ser o plano do spark"
      ],
      "metadata": {
        "id": "Thkh688aSHz3"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "As ações que o spark vai tomar pra execução final são exatamente iguais, tanto sql query como api spark"
      ],
      "metadata": {
        "id": "U-SFw9puSSmt"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "group_sql.show(\n",
        "    \n",
        "\n",
        "group_df.show()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 130
        },
        "id": "LoY2SuhSScCN",
        "outputId": "c36070f0-ea4c-47dd-a0ed-f9576cd87ed2"
      },
      "execution_count": 20,
      "outputs": [
        {
          "output_type": "error",
          "ename": "SyntaxError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;36m  File \u001b[0;32m\"<ipython-input-20-9a0e1d549bd2>\"\u001b[0;36m, line \u001b[0;32m4\u001b[0m\n\u001b[0;31m    group_df.show()\u001b[0m\n\u001b[0m                   ^\u001b[0m\n\u001b[0;31mSyntaxError\u001b[0m\u001b[0;31m:\u001b[0m unexpected EOF while parsing\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "A unica diferença é que foi criada uma nova view"
      ],
      "metadata": {
        "id": "k_jxoeVDSu5m"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Com o spark.sql eu posso executar qualquer tipo de querry"
      ],
      "metadata": {
        "id": "601jY1fSSzH9"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "spark.sql(  \n",
        "      '''\n",
        "        select\n",
        "          chave_pix,\n",
        "          sum(valor)\n",
        "        from transacoes_pix\n",
        "        group by 1\n",
        "        order by 2\n",
        "    '''\n",
        ").show()"
      ],
      "metadata": {
        "id": "p2E1BFgZS_Pt"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "O group by 1 ele vai pegar a primeira selecao que eu fiz e vai agrupar. Assim como pro 2. \n",
        "Essa função acima vai somar os valores que tem dentro da minha chave_pix, posso pegar a média dos valors"
      ],
      "metadata": {
        "id": "x2u3swSFTZ3A"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "spark.sql(  \n",
        "      '''\n",
        "        select\n",
        "          chave_pix,\n",
        "          sum(valor)\n",
        "        from transacoes_pix\n",
        "        group by 1\n",
        "        order by 2\n",
        "    '''\n",
        ").show()"
      ],
      "metadata": {
        "id": "f8ZuZyZvT-02"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Posso arredondar pra duas casas decimais"
      ],
      "metadata": {
        "id": "TSQzSrkwUALk"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "spark.sql(  \n",
        "      '''\n",
        "        select\n",
        "          chave_pix,\n",
        "          round(sum(valor),2) \n",
        "        from transacoes_pix\n",
        "        group by 1\n",
        "        order by 2\n",
        "    '''\n",
        ").show()"
      ],
      "metadata": {
        "id": "DBgr-eVZUECI"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Identificar a quantidade de transacoes que foram feitas para cada chave_pix"
      ],
      "metadata": {
        "id": "9GW137DGUJDL"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "spark.sql(  \n",
        "      '''\n",
        "        select\n",
        "          chave_pix,\n",
        "          count(*)\n",
        "        from transacoes_pix\n",
        "        group by 1\n",
        "        order by 2 desc\n",
        "    '''\n",
        ").show()"
      ],
      "metadata": {
        "id": "YZbvKFzoUPev"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Selecionar apenas pros valores que são maiores que 10 mil"
      ],
      "metadata": {
        "id": "wUu78w6fUft9"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "spark.sql(  \n",
        "      '''\n",
        "        select\n",
        "          chave_pix,\n",
        "          count(*) as count_maior_10\n",
        "        from transacoes_pix\n",
        "        where valor > 10000\n",
        "        group by 1\n",
        "        order by 2 desc\n",
        "    '''\n",
        ").show()"
      ],
      "metadata": {
        "id": "Jcj1gQQaUi99"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "SparkSQL - Window Function"
      ],
      "metadata": {
        "id": "nKB_v4qzUuFx"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Digamos que eu queira saber quais foram as duas transacaoes mais altas para cada banco. Como podemos fazer isso utilizando row_number que faz parte do SparkSQL - Window Function. O row_number cria uma jnela de dados para cada banco e ordeno do valor maior para o menor. O valor maior vai ter index 1 e assim sucessivamente. "
      ],
      "metadata": {
        "id": "60-jS_ygVSsY"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Temos que selecionar o banco do destinatario e tambem saber o valor da transacao. "
      ],
      "metadata": {
        "id": "6aiFsuv8Vyl0"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "spark.sql(\n",
        "    '''\n",
        "      select\n",
        "        destinatario.banco,\n",
        "        valor,\n",
        "        row_number() over (partition by destinatario.banco order by valor desc as row_number\n",
        "        from transacoes_pix\n",
        "        limit 10\n",
        "    '''\n",
        ").show()"
      ],
      "metadata": {
        "id": "oVWTf15fWCwd"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Pegamos por exemplo os 10 maiores transacoes do BTG"
      ],
      "metadata": {
        "id": "2XmWpcArWrOJ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Agora como que eu posso pegar apenas as duas linhas maiores. Teria que fazer um filtro nessa própria consulta. Pra isso uso cte, que são trabelas temporarias criadas a partir de um select para selecionar desse select, ou seja, crio um sub select."
      ],
      "metadata": {
        "id": "KI4zjKtjWy9-"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "spark.sql(\n",
        "    '''\n",
        "      with cte_base_window(\n",
        "      select\n",
        "        banco,\n",
        "        valor,\n",
        "        row_number() over (partition by destinatario.banco order by valor desc as row_number\n",
        "      from transacoes_pix\n",
        "      )  select\n",
        "          banco,\n",
        "          valor\n",
        "        from cte_base_window\n",
        "        where row_number in (1,2)\n",
        "        ''' \n",
        ").show()"
      ],
      "metadata": {
        "id": "guz9NZKBXPsD"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Vamos tirar o sub select e colocar pra um dataframe"
      ],
      "metadata": {
        "id": "wl_INBEEYYFw"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "df_row_number = spark.sql('''\n",
        "  select\n",
        "    destinatario.banco,\n",
        "    valor,\n",
        "    row_number() over (partition by destinatario.banco order by valor desc as row_number\n",
        "  from transacoes_pix\n",
        "    '''\n",
        ")"
      ],
      "metadata": {
        "id": "x8bdiv4YYqFB"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df_row_number.show()"
      ],
      "metadata": {
        "id": "AJwKQiNtZMQp"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from pyspark.sql.functions import col\n",
        "\n",
        "df_row_number.filter(col('row_number').isin([1,2])).show()"
      ],
      "metadata": {
        "id": "rxrSTvI1ZXRy"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [],
      "metadata": {
        "id": "l3DztUbNZz40"
      }
    }
  ]
}