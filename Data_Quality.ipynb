{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyNbdxrWrCjhfB/fser3bzX0",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Epilef86/DNC/blob/main/Data_Quality.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Qualidade dos dados. Um das ferramentas usada é o PyDeequ, objetivo de executar uma qualidade de dados dentro do nosso dataset.\n",
        "\n",
        "A arquitetura do PyDeequ, temos 04 arqiteturas principais:\n",
        "\n",
        "1- Computação de Métricas\n",
        "\n",
        "Profile e Analyzers: que praticamente são algumas análises que passamos em cima do dataset que vão definir se tem qualidade ou não e ele também salva essas metrica no local que definirmos.\n",
        "\n",
        "2- Constraint Suggestion\n",
        "\n",
        "Ele vai olhar pro dataset e vai sugerir algumas métricas de qualidade, as vezes se eu não conheço bem a base de dados, posso usar esse método pra que o prórpio Deequ fazer essa análise\n",
        "\n",
        "\n",
        "3- Constraint Verification\n",
        "\n",
        "Performa a qualidade de dados em cima do dataset, então verificar se a coluna so tem o tipo de dado data, então eu consigo criar através desse método \n",
        "\n",
        "4- Metrics Repository\n",
        "\n",
        "Aonde permite a persistência das métricas em algum lugar, colocando essa métricas em um repositório\n",
        "\n",
        "\n",
        "O Deequ funciona em cima do spark, a analise de qualidade é rápida. Ele pode salvar as métricas no HDFS, AWS S3, na pórpira memória. Com isso eu posso reportes diários, mensais. \n"
      ],
      "metadata": {
        "id": "2AC89NbbNfnS"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Instalando a versão 3.0 do PySpark"
      ],
      "metadata": {
        "id": "Q6U7WphIpJsn"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install pyspark==3.0.3"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_JYt2IrTq5J0",
        "outputId": "3a04e83f-47da-45b1-9a72-fea7e676a1cc"
      },
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Collecting pyspark==3.0.3\n",
            "  Downloading pyspark-3.0.3.tar.gz (209.1 MB)\n",
            "\u001b[2K     \u001b[91m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[90m╺\u001b[0m\u001b[90m━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m110.1/209.1 MB\u001b[0m \u001b[31m118.5 MB/s\u001b[0m eta \u001b[36m0:00:01\u001b[0m\n",
            "\u001b[?25h\u001b[31mERROR: Exception:\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.8/dist-packages/pip/_vendor/urllib3/response.py\", line 438, in _error_catcher\n",
            "    yield\n",
            "  File \"/usr/local/lib/python3.8/dist-packages/pip/_vendor/urllib3/response.py\", line 519, in read\n",
            "    data = self._fp.read(amt) if not fp_closed else b\"\"\n",
            "  File \"/usr/local/lib/python3.8/dist-packages/pip/_vendor/cachecontrol/filewrapper.py\", line 90, in read\n",
            "    data = self.__fp.read(amt)\n",
            "  File \"/usr/lib/python3.8/http/client.py\", line 459, in read\n",
            "    n = self.readinto(b)\n",
            "  File \"/usr/lib/python3.8/http/client.py\", line 503, in readinto\n",
            "    n = self.fp.readinto(b)\n",
            "  File \"/usr/lib/python3.8/socket.py\", line 669, in readinto\n",
            "    return self._sock.recv_into(b)\n",
            "  File \"/usr/lib/python3.8/ssl.py\", line 1241, in recv_into\n",
            "    return self.read(nbytes, buffer)\n",
            "  File \"/usr/lib/python3.8/ssl.py\", line 1099, in read\n",
            "    return self._sslobj.read(len, buffer)\n",
            "socket.timeout: The read operation timed out\n",
            "\n",
            "During handling of the above exception, another exception occurred:\n",
            "\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.8/dist-packages/pip/_internal/cli/base_command.py\", line 167, in exc_logging_wrapper\n",
            "    status = run_func(*args)\n",
            "  File \"/usr/local/lib/python3.8/dist-packages/pip/_internal/cli/req_command.py\", line 199, in wrapper\n",
            "    return func(self, options, args)\n",
            "  File \"/usr/local/lib/python3.8/dist-packages/pip/_internal/commands/install.py\", line 339, in run\n",
            "    requirement_set = resolver.resolve(\n",
            "  File \"/usr/local/lib/python3.8/dist-packages/pip/_internal/resolution/resolvelib/resolver.py\", line 94, in resolve\n",
            "    result = self._result = resolver.resolve(\n",
            "  File \"/usr/local/lib/python3.8/dist-packages/pip/_vendor/resolvelib/resolvers.py\", line 481, in resolve\n",
            "    state = resolution.resolve(requirements, max_rounds=max_rounds)\n",
            "  File \"/usr/local/lib/python3.8/dist-packages/pip/_vendor/resolvelib/resolvers.py\", line 348, in resolve\n",
            "    self._add_to_criteria(self.state.criteria, r, parent=None)\n",
            "  File \"/usr/local/lib/python3.8/dist-packages/pip/_vendor/resolvelib/resolvers.py\", line 172, in _add_to_criteria\n",
            "    if not criterion.candidates:\n",
            "  File \"/usr/local/lib/python3.8/dist-packages/pip/_vendor/resolvelib/structs.py\", line 151, in __bool__\n",
            "    return bool(self._sequence)\n",
            "  File \"/usr/local/lib/python3.8/dist-packages/pip/_internal/resolution/resolvelib/found_candidates.py\", line 155, in __bool__\n",
            "    return any(self)\n",
            "  File \"/usr/local/lib/python3.8/dist-packages/pip/_internal/resolution/resolvelib/found_candidates.py\", line 143, in <genexpr>\n",
            "    return (c for c in iterator if id(c) not in self._incompatible_ids)\n",
            "  File \"/usr/local/lib/python3.8/dist-packages/pip/_internal/resolution/resolvelib/found_candidates.py\", line 47, in _iter_built\n",
            "    candidate = func()\n",
            "  File \"/usr/local/lib/python3.8/dist-packages/pip/_internal/resolution/resolvelib/factory.py\", line 215, in _make_candidate_from_link\n",
            "    self._link_candidate_cache[link] = LinkCandidate(\n",
            "  File \"/usr/local/lib/python3.8/dist-packages/pip/_internal/resolution/resolvelib/candidates.py\", line 288, in __init__\n",
            "    super().__init__(\n",
            "  File \"/usr/local/lib/python3.8/dist-packages/pip/_internal/resolution/resolvelib/candidates.py\", line 158, in __init__\n",
            "    self.dist = self._prepare()\n",
            "  File \"/usr/local/lib/python3.8/dist-packages/pip/_internal/resolution/resolvelib/candidates.py\", line 227, in _prepare\n",
            "    dist = self._prepare_distribution()\n",
            "  File \"/usr/local/lib/python3.8/dist-packages/pip/_internal/resolution/resolvelib/candidates.py\", line 299, in _prepare_distribution\n",
            "    return preparer.prepare_linked_requirement(self._ireq, parallel_builds=True)\n",
            "  File \"/usr/local/lib/python3.8/dist-packages/pip/_internal/operations/prepare.py\", line 487, in prepare_linked_requirement\n",
            "    return self._prepare_linked_requirement(req, parallel_builds)\n",
            "  File \"/usr/local/lib/python3.8/dist-packages/pip/_internal/operations/prepare.py\", line 532, in _prepare_linked_requirement\n",
            "    local_file = unpack_url(\n",
            "  File \"/usr/local/lib/python3.8/dist-packages/pip/_internal/operations/prepare.py\", line 214, in unpack_url\n",
            "    file = get_http_url(\n",
            "  File \"/usr/local/lib/python3.8/dist-packages/pip/_internal/operations/prepare.py\", line 94, in get_http_url\n",
            "    from_path, content_type = download(link, temp_dir.path)\n",
            "  File \"/usr/local/lib/python3.8/dist-packages/pip/_internal/network/download.py\", line 146, in __call__\n",
            "    for chunk in chunks:\n",
            "  File \"/usr/local/lib/python3.8/dist-packages/pip/_internal/cli/progress_bars.py\", line 304, in _rich_progress_bar\n",
            "    for chunk in iterable:\n",
            "  File \"/usr/local/lib/python3.8/dist-packages/pip/_internal/network/utils.py\", line 63, in response_chunks\n",
            "    for chunk in response.raw.stream(\n",
            "  File \"/usr/local/lib/python3.8/dist-packages/pip/_vendor/urllib3/response.py\", line 576, in stream\n",
            "    data = self.read(amt=amt, decode_content=decode_content)\n",
            "  File \"/usr/local/lib/python3.8/dist-packages/pip/_vendor/urllib3/response.py\", line 541, in read\n",
            "    raise IncompleteRead(self._fp_bytes_read, self.length_remaining)\n",
            "  File \"/usr/lib/python3.8/contextlib.py\", line 131, in __exit__\n",
            "    self.gen.throw(type, value, traceback)\n",
            "  File \"/usr/local/lib/python3.8/dist-packages/pip/_vendor/urllib3/response.py\", line 443, in _error_catcher\n",
            "    raise ReadTimeoutError(self._pool, None, \"Read timed out.\")\n",
            "pip._vendor.urllib3.exceptions.ReadTimeoutError: HTTPSConnectionPool(host='files.pythonhosted.org', port=443): Read timed out.\u001b[0m\u001b[31m\n",
            "\u001b[0m"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!wget -qnc https://bin.equinox.io/c/4VmDzA7iaHb/ngrok-stable-linux-amd64.zip\n",
        "!unzip -n -q ngrok-stable-linux-amd64.zip "
      ],
      "metadata": {
        "id": "fqBE-Dy1q-QG"
      },
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!./ngrok authtoken 2Lh7Zu0kns54bTSQCTrDDYkyvNH_71s7E3phX7wCyU4nuAFpH\n",
        "get_ipython().system_raw('./ngrok http 4050 &')\n",
        "!sleep 10\n",
        "!curl -s http://localhost:4040/api/tunnels | grep -Po 'public_url\":\"(?=https)\\K[^\"]*'"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "9CX1GSTKrBbk",
        "outputId": "2b33233a-2576-4354-e27d-ac9536e0fc94"
      },
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Authtoken saved to configuration file: /root/.ngrok2/ngrok.yml\n",
            "https://d9dd-34-80-60-76.ngrok.io\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "\n",
        "drive.mount('/content/drive')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "aazVfGu_rHD8",
        "outputId": "2cb7567b-42cc-46f2-ce47-df34f2ab1c2d"
      },
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install pydeequ"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "za0ydV8Rr1IT",
        "outputId": "0e9d5151-7456-4d13-f4a8-4b0a9fcec9f8"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Collecting pydeequ\n",
            "  Downloading pydeequ-1.0.1-py3-none-any.whl (36 kB)\n",
            "Requirement already satisfied: pandas>=0.23.0 in /usr/local/lib/python3.8/dist-packages (from pydeequ) (1.3.5)\n",
            "Requirement already satisfied: numpy>=1.14.1 in /usr/local/lib/python3.8/dist-packages (from pydeequ) (1.22.4)\n",
            "Requirement already satisfied: python-dateutil>=2.7.3 in /usr/local/lib/python3.8/dist-packages (from pandas>=0.23.0->pydeequ) (2.8.2)\n",
            "Requirement already satisfied: pytz>=2017.3 in /usr/local/lib/python3.8/dist-packages (from pandas>=0.23.0->pydeequ) (2022.7.1)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.8/dist-packages (from python-dateutil>=2.7.3->pandas>=0.23.0->pydeequ) (1.15.0)\n",
            "Installing collected packages: pydeequ\n",
            "Successfully installed pydeequ-1.0.1\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from pyspark.sql import SparkSession\n",
        "\n",
        "import pydeequ\n",
        "\n",
        "spark = (\n",
        "    SparkSession.builder\n",
        "    .config('spark.ui.port', '4050')\n",
        "    .config('spark.jars.packages', pydeequ.deequ_maven_coord)\n",
        "    .config('spark.jars.excludes', pydeequ.f2j_maven_coord)\n",
        "    .appName(\"SparkSQL\")\n",
        "    .getOrCreate()\n",
        ")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 373
        },
        "id": "7E9OYcOxr9PB",
        "outputId": "062ce7bc-85d8-49b9-c1ca-9bdbbab21121"
      },
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "error",
          "ename": "ModuleNotFoundError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-11-a40b3b2a3cd9>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0mpyspark\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msql\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mSparkSession\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mpydeequ\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m spark = (\n",
            "\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'pyspark'",
            "",
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0;32m\nNOTE: If your import is failing due to a missing package, you can\nmanually install dependencies using either !pip or !apt.\n\nTo view examples of installing some common dependencies, click the\n\"Open Examples\" button below.\n\u001b[0;31m---------------------------------------------------------------------------\u001b[0m\n"
          ],
          "errorDetails": {
            "actions": [
              {
                "action": "open_url",
                "actionText": "Open Examples",
                "url": "/notebooks/snippets/importing_libraries.ipynb"
              }
            ]
          }
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Usando o mesmo arquilo do sparkML"
      ],
      "metadata": {
        "id": "r3ek6Y7ctd2o"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from pyspark.sql.types import StructType, StructField, IntegerType, DoubleType, StringType, TimestampType \n",
        "from pyspark.sql.functions import col\n",
        "\n",
        "schema_remetente_destinatario = StructType([\n",
        "    StructField('nome', StringType()),\n",
        "    StructField('banco', StringType()),\n",
        "    StructField('tipo', StringType())\n",
        "])\n",
        "\n",
        "schema_base_pix = StructType([\n",
        "    StructField('id_transacao', IntegerType()),\n",
        "    StructField('valor', DoubleType()),\n",
        "    StructField('remetente ', schema_remetente_destinatario),\n",
        "    StructField('destinatario', schema_remetente_destinatario),\n",
        "    StructField('transaction_date', StringType()),\n",
        "    StructField('chave_pix', StringType()),\n",
        "    StructField('categoria', StringType()),\n",
        "    StructField('fraude', IntegerType())\n",
        "])\n",
        "\n",
        "caminho_json = 'drive/MyDrive/Colab Notebooks/case_final.json'\n",
        "\n",
        "df = spark.read.json(\n",
        "    caminho_json,\n",
        "    schema = schema_base_pix,\n",
        "    timestampFormat = \"yyyy-MM-dd HH:mm:ss\"\n",
        ")"
      ],
      "metadata": {
        "id": "tPFzdsKety5x"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "O Deequ não trabalha com estruturas, então precisamos fazer o flat desses dados"
      ],
      "metadata": {
        "id": "7h1Wb05Dt_k9"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "df = df.withColumn(\n",
        "    'destinatario_nome', col('destinatario').getField('nome')\n",
        "    ).withColumn(\n",
        "    'destinatario_banco', col('destinatario').getField('banco')\n",
        "    ).withColumn(\n",
        "    'destinatario_tipo', col('destinatario').getField('tipo')\n",
        "    ).withColumn(\n",
        "    'remetente_nome', col('remetente').getField('nome')\n",
        "    ).withColumn(\n",
        "    'remetente_banco', col('remetente').getField('banco')\n",
        "    ).withColumn(\n",
        "    'remetente_tipo', col('remetente').getField('tipo')\n",
        ").drop('remetente', 'destinatario')"
      ],
      "metadata": {
        "id": "Wljv73sDugrB"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "nto alguns analiyzers"
      ],
      "metadata": {
        "id": "_YW6f7MYvryV"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from pydeequ.analyzers import AnalysisRunner, AnalyzerContext, ApproxCountDistinct, Completeness, Compliance, Mean, Size"
      ],
      "metadata": {
        "id": "Jt9-7Ltmvu1U"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Analisar coluna que ID_transação precisa ser completa, pra analisar isso dentro do codigo, precisamos criar o retorno de nalises que vai ser a variavel que vai guardar  resultado da analise"
      ],
      "metadata": {
        "id": "kx-J4zLwwZzp"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "analisisResult = (\n",
        "    AnalysisRunner(spark).onData(df)\n",
        "    .addAnalyzer(Size())\n",
        "    .addAnalyzer(Completeness('id_transacao'))\n",
        "    .addAnalyzer(Compliance('valor','valor > 0'))\n",
        "    .run()\n",
        ")"
      ],
      "metadata": {
        "id": "aP3sa2HawTyk"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Pra printar precisamos transformar pra um dataframe"
      ],
      "metadata": {
        "id": "L0n-GZTQvnNU"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "analysisResult_df = AnalyzerContext.sucessMetricsAsDataFrame(spark,analysisResult)"
      ],
      "metadata": {
        "id": "USDnvQ7cx54k"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "analysisResult_df.show()"
      ],
      "metadata": {
        "id": "yozKpHQhyPfW"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Agora podemos ver quais foram os resultados dessas execuções de qualidade. \n",
        "Por exemplo Size vimos que o nosso dataset tem 100000 linhas\n",
        "id_transacao 1.0 ou seja completo, 100% de casos que o ID está preenchido\n",
        "\n",
        "Complice 0.99972, ou seja, esse é a porcentagem que a condição valor > 0, determinada anteriormente é certo."
      ],
      "metadata": {
        "id": "HbcdauLWye1g"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Digamos que eu n consiga descrever todos os analyzers que eu quero pq não conheco muto bem o dataset. Então o deequ fornece o ConstraintSuggestionRunner, ou seja, ele vai sugerir algumas métricas de qualidade. Para isso, precisamos passar o contexto que o spark tem, qual o daataframe e o addConstrainRule e o run é pra executar esse código."
      ],
      "metadata": {
        "id": "QNhbPty9z3ww"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from pydeequ.suggestions import ConstrainSuggestionRunner, DEFAULT\n",
        "\n",
        "suggestionResult = ConstrainSuggestionRunner(spark).onData(df).addConstraintRule(DEFAULT()).run()"
      ],
      "metadata": {
        "id": "5PDSUdIr0mKv"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Na proxima linha foi eito um for loop que vai pegar linha a linha das susgestões que ele der e a gente printar qual foi a susgetão e qual o codigo spark já que ele nos retorna executar a qualidade."
      ],
      "metadata": {
        "id": "I5qPoAMr1GYQ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Podermos verifica que nunca esta nulo o campo, já podemos adicionar .isComplete "
      ],
      "metadata": {
        "id": "wb3HdiiI1cgd"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Podemos adicionar um check especifico no df, ou seja, ele vai checar se o df vai dar erro ou se nçai vai continuar Pra isso precisamos criar duas variaveis. Primeiro check ue tem nivel warning, ou seja se ele ver que está com warning e não para o programa, quando e=é error é que não pode acontecer, ele para o programa"
      ],
      "metadata": {
        "id": "tL-KN2Po13-a"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from pydeequ.checks import Check, CheckLevels, ConstrainableDataTypes\n",
        "from pydeequ.verification import VerificationResult, VerificationSuite\n",
        "\n",
        "check = Check(spark, CheckLevel.Warning, \"Review Check\")\n",
        "error = Check(spark, CheckLevel.Error, 'Error')"
      ],
      "metadata": {
        "id": "5fHMCslb2Mpw"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Podemos cria um check result, nos temos que passar a sessão spark, qual dataframe que vai ser analisado e adicionar um check especifico. Perceba que eu estou criando atraves da variavel check, ele so vai falar pra prestar atencao aos dados mas não vai retornar uma excessao. Então eu to falando que id_trsacao seja um inteiro, não pode vim string, não pode vim double, também n posso ter id_trsacao negativa, não posso ter valo nulo"
      ],
      "metadata": {
        "id": "I-D-B_CU3Vky"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "checkResult = (\n",
        "    VerificationSuite(spark)\n",
        "    .onData(df)\n",
        "    .addCheck(\n",
        "        check.hasDataType('id_transacao',ConstrainableDataTypes.Integral)\n",
        "        .isNonNegative('id_transacao')\n",
        "        .isComplete('id_transacao')\n",
        "    )\n",
        "  .run()\n",
        ")"
      ],
      "metadata": {
        "id": "KGww5_G43YSz"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Agora podemos verificar se o data frame se realmente tem a metrica de qualidade através da função VerificationResult, CheckResultAsDataframe, ele já vai retorna o resultado como dataframe e ele vai falar que o Review check está como nível warning, ele está como sucess, ou seja, nao deu falhas, ou seja, todos os tipos de id_trasacao sao realmente inteiros, não sao negativos e nao nulos"
      ],
      "metadata": {
        "id": "jmo0fzda4K4J"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "checkResult_df = VerificationResult.checkResultsAsDataFrame(spark, checkResult)\n",
        "checkResult_df.show(truncate=False)"
      ],
      "metadata": {
        "id": "jYWOgLRr8vKi"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Agora digamos que a gente queira colocar um erro proposital"
      ],
      "metadata": {
        "id": "koT0Dq2E8EvF"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "checkResult = (\n",
        "    VerificationSuite(spark)\n",
        "    .onData(df)\n",
        "    .addCheck(\n",
        "        error\n",
        "          .isContainedIn('remetente_tipo', ['CNPJ'])\n",
        "    )\n",
        "  .run()\n",
        ")"
      ],
      "metadata": {
        "id": "33bALMDo8Tia"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "checkResult_df = VerificationResult.checkResultsAsDataFrame(spark, checkResult)\n",
        "checkResult_df.show(truncate=False)"
      ],
      "metadata": {
        "id": "KUXhCpui8jrW"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "O resultado deu erro pq o valor não está de acordo com o que foi passado, era pra ter apenas CNPJ, porém vem do tipo PJ ou PF. Ele vai me falar qual o erro e o que devo fazer"
      ],
      "metadata": {
        "id": "mWVlI9UL8h9g"
      }
    }
  ]
}