{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyMLajsYHa4bCBMPov8ScWgw",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Epilef86/DNC/blob/main/Spark_UI.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Spark Ui é o local aonde posso ver todas as informações do meu job. Através do spark UI posso verificar o progresso dos meus jobs, quanto tempo foi executado, em qual parte travou, posso ver também algumas variáveis de ambiente"
      ],
      "metadata": {
        "id": "XVTcGwC3pEZN"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Na primeira parte quando abre a página do spark UI temos quais jobs estão sendo executados. \n",
        "\n",
        "Job se refere a um processo de execução de algum cálculo de processamento em um conjunto de dados, ou seja, um job é aquele processamento que você mandou um job executar. \n",
        "\n",
        "Stage é um conjunto de tarefas que podem ser executadas em paralelo que faz com que o job aconteça. Na aba de stage mostra quais foram as tarefas que foram realizadas. Você pode ta tendo uma noção de quanto tempoestá durando seu job, as taks.\n",
        "\n",
        "task é a menor unidade de trabalho que podemos ter no spark, é a execução em paralelo, logo posso ter várias tasks em paralelo.\n",
        "\n",
        "Pra ficar claro, job é o objetivo final pra se conquistar com seu código. Stage são vários conjuntos de taks que são rodados em paralelo. E task ão os processamentos que podem ser rodados em paralelo.\n",
        "\n",
        "N aba de dataframe, mostra quantos arquivos foram lidos, depois mostra que foi realizado um filtro nos dados, depois o HashAggregate que nada mais é do que executar um processamento em cima do dataset e depois vai mostrando vários passos que o spark toma até o resultado final.\n",
        "\n",
        "No deils, vemos o plano físico que o spark gerou, mostra o passo a passo do que foi planejado antes de executar. \n",
        "\n",
        "Na aba executors pode verificar quais são os executores da que estão na máquina , o colab trabalha com duas CPUs, ou seja, duas cores. "
      ],
      "metadata": {
        "id": "idzhKJjmpqnI"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "0DnT1huMo8P6"
      },
      "outputs": [],
      "source": []
    }
  ]
}