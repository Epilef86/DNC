{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyOrZBHLAfbtdSsAFXt3/+6i",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Epilef86/DNC/blob/main/SparkML.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "  A vantagem de usar spark ML pra fazer execução de algoritmo de machine learn é que podemos distribuir e processar esses dados de forma paralela.O spark ML faz os algotimos e suas previsões, cluesterizações de forma paralela. Outra vantagem é que ele permite que trabalhe não só com python, podemos executar esses algotimos usando R, java, diversas liguagens de programação que o spark nos provém.  "
      ],
      "metadata": {
        "id": "j7Nj5rKJz2ot"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Vamos tentar ver se há alguma fraude na base de transações"
      ],
      "metadata": {
        "id": "FteJq4te1Xfn"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "1ª coisa é preparar o ambiente"
      ],
      "metadata": {
        "id": "kwNq-rag1h6Z"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install pyspark"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "9vazpsaO12mf",
        "outputId": "f6c8e930-080b-4a31-ca1a-e0b2e40e0330"
      },
      "execution_count": 20,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Requirement already satisfied: pyspark in /usr/local/lib/python3.8/dist-packages (3.3.2)\n",
            "Requirement already satisfied: py4j==0.10.9.5 in /usr/local/lib/python3.8/dist-packages (from pyspark) (0.10.9.5)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "2º Passo: para acessar o sparkui através do colba, é necessário instalar o ngrok, que é uma ferramenta que possibilita que acesse através de um servidor que está no google colab."
      ],
      "metadata": {
        "id": "6QSDT5Dn2AUg"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!wget -qnc https://bin.equinox.io/c/4VmDzA7iaHb/ngrok-stable-linux-amd64.zip\n",
        "!unzip -n -q ngrok-stable-linux-amd64.zip "
      ],
      "metadata": {
        "id": "KJSvIp9t14cj"
      },
      "execution_count": 21,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Quando der um refresh os arquivos vão aparecer ao lado, ou seja, o ngrok já está na instalado na máquina"
      ],
      "metadata": {
        "id": "R86KaLk62Eul"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from pyspark.sql import SparkSession\n",
        "\n",
        "spark = (\n",
        "    SparkSession.builder\n",
        "    .config('spark.ui.port', '4050')\n",
        "    .appName(\"SparkUI Introdução\")\n",
        "    .getOrCreate()\n",
        ")"
      ],
      "metadata": {
        "id": "58c16pXp2ITt"
      },
      "execution_count": 22,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "A única configuração que vai ser colocado a mais é que a porta do sparkui estará na 4050, enquanto padrão (default) é 4040. Faz essa mudança por orientação ao ngrok pq ele mesmo usa essa porta."
      ],
      "metadata": {
        "id": "Q1gi5qqA2Oi2"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!./ngrok authtoken 2Lh7Zu0kns54bTSQCTrDDYkyvNH_71s7E3phX7wCyU4nuAFpH\n",
        "get_ipython().system_raw('./ngrok http 4050 &')\n",
        "!sleep 10\n",
        "!curl -s http://localhost:4040/api/tunnels | grep -Po 'public_url\":\"(?=https)\\K[^\"]*'"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "nz3lGUOk2Ryn",
        "outputId": "ca475e57-a224-4ce5-8d54-963f7f9d44f6"
      },
      "execution_count": 23,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Authtoken saved to configuration file: /root/.ngrok2/ngrok.yml\n",
            "https://f206-34-86-124-12.ngrok.io\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Para conseguir pegar os dados direto do drive pq a base de dados é relativamente grande, tem cerca de 25 M, então demora muito pra subir pro colab. Então dessa maneira podemos autenticar no drive, basta conectar a conta do drive, permitir que ele acesse. "
      ],
      "metadata": {
        "id": "l55dQu-t2cVb"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "CbP9wq1n3Azf",
        "outputId": "1fc07423-da9e-48fd-e75e-435c8be9040e"
      },
      "execution_count": 24,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "1ª passo é ler os dados que precisamos aplicar ao nosso modelo"
      ],
      "metadata": {
        "id": "7Vv3GWMl4qiX"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from pyspark.sql.types import StructType, StructField, IntegerType, DoubleType, StringType, TimestampType \n",
        "\n",
        "schema_remetente_destinatario = StructType([\n",
        "    StructField('nome', StringType()),\n",
        "    StructField('banco', StringType()),\n",
        "    StructField('tipo', StringType())\n",
        "])\n",
        "\n",
        "schema_base_pix = StructType([\n",
        "    StructField('id_transacao', IntegerType()),\n",
        "    StructField('valor', DoubleType()),\n",
        "    StructField('remetente ', schema_remetente_destinatario),\n",
        "    StructField('destinatario', schema_remetente_destinatario),\n",
        "    StructField('transaction_date', StringType()),\n",
        "    StructField('chave_pix', StringType()),\n",
        "    StructField('categoria', StringType()),\n",
        "    StructField('fraude', IntegerType())\n",
        "])\n",
        "\n",
        "caminho_json = 'drive/MyDrive/Colab Notebooks/case_final.json'\n",
        "\n",
        "df = spark.read.json(\n",
        "    caminho_json,\n",
        "    schema = schema_base_pix,\n",
        "    timestampFormat = \"yyyy-MM-dd HH:mm:ss\"\n",
        ")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "dDl9z6ZW4_VV",
        "outputId": "82cb261e-cba7-491a-a2f1-9669e4b4c152"
      },
      "execution_count": 29,
      "outputs": [
        {
          "output_type": "error",
          "ename": "Py4JJavaError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mPy4JJavaError\u001b[0m                             Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-29-baa4263623e9>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     20\u001b[0m \u001b[0mcaminho_json\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m'https://drive.google.com/drive/u/0/my-drive'\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     21\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 22\u001b[0;31m df = spark.read.json(\n\u001b[0m\u001b[1;32m     23\u001b[0m     \u001b[0mcaminho_json\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     24\u001b[0m     \u001b[0mschema\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mschema_base_pix\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.8/dist-packages/pyspark/sql/readwriter.py\u001b[0m in \u001b[0;36mjson\u001b[0;34m(self, path, schema, primitivesAsString, prefersDecimal, allowComments, allowUnquotedFieldNames, allowSingleQuotes, allowNumericLeadingZero, allowBackslashEscapingAnyCharacter, mode, columnNameOfCorruptRecord, dateFormat, timestampFormat, multiLine, allowUnquotedControlChars, lineSep, samplingRatio, dropFieldIfAllNull, encoding, locale, pathGlobFilter, recursiveFileLookup, modifiedBefore, modifiedAfter, allowNonNumericNumbers)\u001b[0m\n\u001b[1;32m    282\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mtype\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0mlist\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    283\u001b[0m             \u001b[0;32massert\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_spark\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_sc\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_jvm\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 284\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_df\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_jreader\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mjson\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_spark\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_sc\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_jvm\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mPythonUtils\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtoSeq\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    285\u001b[0m         \u001b[0;32melif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mRDD\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    286\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.8/dist-packages/py4j/java_gateway.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args)\u001b[0m\n\u001b[1;32m   1319\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1320\u001b[0m         \u001b[0manswer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgateway_client\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msend_command\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcommand\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1321\u001b[0;31m         return_value = get_return_value(\n\u001b[0m\u001b[1;32m   1322\u001b[0m             answer, self.gateway_client, self.target_id, self.name)\n\u001b[1;32m   1323\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.8/dist-packages/pyspark/sql/utils.py\u001b[0m in \u001b[0;36mdeco\u001b[0;34m(*a, **kw)\u001b[0m\n\u001b[1;32m    188\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mdeco\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0ma\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mAny\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkw\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mAny\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0mAny\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    189\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 190\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0ma\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkw\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    191\u001b[0m         \u001b[0;32mexcept\u001b[0m \u001b[0mPy4JJavaError\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    192\u001b[0m             \u001b[0mconverted\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mconvert_exception\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mjava_exception\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.8/dist-packages/py4j/protocol.py\u001b[0m in \u001b[0;36mget_return_value\u001b[0;34m(answer, gateway_client, target_id, name)\u001b[0m\n\u001b[1;32m    324\u001b[0m             \u001b[0mvalue\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mOUTPUT_CONVERTER\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mtype\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0manswer\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgateway_client\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    325\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0manswer\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0mREFERENCE_TYPE\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 326\u001b[0;31m                 raise Py4JJavaError(\n\u001b[0m\u001b[1;32m    327\u001b[0m                     \u001b[0;34m\"An error occurred while calling {0}{1}{2}.\\n\"\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    328\u001b[0m                     format(target_id, \".\", name), value)\n",
            "\u001b[0;31mPy4JJavaError\u001b[0m: An error occurred while calling o83.json.\n: java.lang.UnsupportedOperationException\n\tat org.apache.hadoop.fs.http.AbstractHttpFileSystem.listStatus(AbstractHttpFileSystem.java:95)\n\tat org.apache.hadoop.fs.http.HttpsFileSystem.listStatus(HttpsFileSystem.java:23)\n\tat org.apache.spark.util.HadoopFSUtils$.listLeafFiles(HadoopFSUtils.scala:225)\n\tat org.apache.spark.util.HadoopFSUtils$.$anonfun$parallelListLeafFilesInternal$1(HadoopFSUtils.scala:95)\n\tat scala.collection.TraversableLike.$anonfun$map$1(TraversableLike.scala:286)\n\tat scala.collection.mutable.ResizableArray.foreach(ResizableArray.scala:62)\n\tat scala.collection.mutable.ResizableArray.foreach$(ResizableArray.scala:55)\n\tat scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:49)\n\tat scala.collection.TraversableLike.map(TraversableLike.scala:286)\n\tat scala.collection.TraversableLike.map$(TraversableLike.scala:279)\n\tat scala.collection.AbstractTraversable.map(Traversable.scala:108)\n\tat org.apache.spark.util.HadoopFSUtils$.parallelListLeafFilesInternal(HadoopFSUtils.scala:85)\n\tat org.apache.spark.util.HadoopFSUtils$.parallelListLeafFiles(HadoopFSUtils.scala:69)\n\tat org.apache.spark.sql.execution.datasources.InMemoryFileIndex$.bulkListLeafFiles(InMemoryFileIndex.scala:158)\n\tat org.apache.spark.sql.execution.datasources.InMemoryFileIndex.listLeafFiles(InMemoryFileIndex.scala:131)\n\tat org.apache.spark.sql.execution.datasources.InMemoryFileIndex.refresh0(InMemoryFileIndex.scala:94)\n\tat org.apache.spark.sql.execution.datasources.InMemoryFileIndex.<init>(InMemoryFileIndex.scala:66)\n\tat org.apache.spark.sql.execution.datasources.DataSource.createInMemoryFileIndex(DataSource.scala:567)\n\tat org.apache.spark.sql.execution.datasources.DataSource.resolveRelation(DataSource.scala:409)\n\tat org.apache.spark.sql.DataFrameReader.loadV1Source(DataFrameReader.scala:228)\n\tat org.apache.spark.sql.DataFrameReader.$anonfun$load$2(DataFrameReader.scala:210)\n\tat scala.Option.getOrElse(Option.scala:189)\n\tat org.apache.spark.sql.DataFrameReader.load(DataFrameReader.scala:210)\n\tat org.apache.spark.sql.DataFrameReader.json(DataFrameReader.scala:361)\n\tat java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\n\tat java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)\n\tat java.base/jdk.internal.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\n\tat java.base/java.lang.reflect.Method.invoke(Method.java:566)\n\tat py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)\n\tat py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:357)\n\tat py4j.Gateway.invoke(Gateway.java:282)\n\tat py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)\n\tat py4j.commands.CallCommand.execute(CallCommand.java:79)\n\tat py4j.ClientServerConnection.waitForCommands(ClientServerConnection.java:182)\n\tat py4j.ClientServerConnection.run(ClientServerConnection.java:106)\n\tat java.base/java.lang.Thread.run(Thread.java:829)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Podemos importar todos os tipos que o spark tem, sem precisar referenciar um a um:\n",
        "\n",
        "from pyspark.sql.types import *"
      ],
      "metadata": {
        "id": "Z7Aw8Cvn7iyj"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "2º Passo: ver o que tem dentro do arquivo"
      ],
      "metadata": {
        "id": "s7D0g3jp_bLj"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "df.printSchema()\n",
        "\n",
        "df.show()"
      ],
      "metadata": {
        "id": "P9LbMn2O_kxM"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Precisamos verificar quais colunas preciso pra fazer o modelo, descarto as que não fazem diferença no modelo"
      ],
      "metadata": {
        "id": "0KWT6s1K_ogY"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Vamos excluir as colunas que não servem pro modelo e  transformar o que era estrutura em novas colunas, que é justamente fazer o flatten. Pra isso preciso criar um dicionário aonde o primeiro parâmetro é o nome da nova coluna e o 2º parametro é o valor dessa coluna"
      ],
      "metadata": {
        "id": "l8lnkVoXAK3H"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Precisamos importar a função de col"
      ],
      "metadata": {
        "id": "K77bn8XUCARF"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "A função drop é pra excluir a coluna "
      ],
      "metadata": {
        "id": "_C4JldqJFbA8"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from pyspark.sql.functions import *\n",
        "\n",
        "df_flatten = df.withColumns({\n",
        "    'destinatario_nome': col('destinatario').getField('nome'),\n",
        "    'destinatario_banco': col('destinatario').getField('banco'),\n",
        "    'destinatario_tipo': col('destinatario').getField('tipo')\n",
        "}).drop('remetente', 'destinatario')"
      ],
      "metadata": {
        "id": "y2Ff2tBeAnL0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "O .getField vai dentro de uma estrutura e pega o valor referente ao valor passado. Ou seja, vai no dicionário e pega o valor dentro da chave"
      ],
      "metadata": {
        "id": "I8ED0juzBbW1"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Verificando como ficou o schema dessa estrutura"
      ],
      "metadata": {
        "id": "myuGh7sYFh5x"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "df_flatten.printSchema()"
      ],
      "metadata": {
        "id": "3V-oxP9kFnsr"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Como a coluna de destinatário foi duplicado, podemos também dropar a coluna de destinatário como mostrado acima"
      ],
      "metadata": {
        "id": "ArGo7SnzFw7R"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Agora vamos fazer a modelagem aplicando algoritmo de machine learn. "
      ],
      "metadata": {
        "id": "s2U5M3yCGDwV"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Na coluna chave_pix temos valores categoricos, o ideal é que a coluna fique numerica para depois poder aplicar regressão logistica."
      ],
      "metadata": {
        "id": "Z4n81Nv1G8cl"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Usando uma biblioteca do pyspark pra pegar um conjunto de colunas de entradas, tenho que referenciar qual as saidas dessas colunas e ele transforma esssas colunas categoricas em valores numericos"
      ],
      "metadata": {
        "id": "KDnjdQSdIPfR"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from pyspark.ml.feature import StringIndexer\n",
        "\n",
        "indexer = StringIndexer(\n",
        "    inputCols=[\n",
        "        'destinatario_nome',\n",
        "        'destinatario_banco',\n",
        "        'destinatario_tipo',\n",
        "        'categoria',\n",
        "        'chave_pix'\n",
        "     ],\n",
        "     outputCols=[\n",
        "        'destinatario_nome_index',\n",
        "        'destinatario_banco_index',\n",
        "        'destinatario_tipo_index',\n",
        "        'categoria_index',\n",
        "     _  'chave_pix_index'\n",
        "     ]\n",
        ")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 130
        },
        "id": "bydk_04JHfQs",
        "outputId": "8ea7b1f7-215e-47f5-e6b4-b40594f1073b"
      },
      "execution_count": 31,
      "outputs": [
        {
          "output_type": "error",
          "ename": "SyntaxError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;36m  File \u001b[0;32m\"<ipython-input-31-213f40e9928f>\"\u001b[0;36m, line \u001b[0;32m16\u001b[0m\n\u001b[0;31m    _  'chave_pix_index'\u001b[0m\n\u001b[0m       ^\u001b[0m\n\u001b[0;31mSyntaxError\u001b[0m\u001b[0;31m:\u001b[0m invalid syntax\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Agora preciso transformar os dados em dataframe. Usar a função fit que vai receber como parametro o dataframe que vai ser trasformado."
      ],
      "metadata": {
        "id": "2SR0wooBJviS"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "df_index = indexer.fit(df_flatton).transform(df_flatton)"
      ],
      "metadata": {
        "id": "-v19C_fSJ0FU"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Para ver se funcionou, vemos qual schema do data frame novo"
      ],
      "metadata": {
        "id": "QiWae2iAKN2k"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "df_index.printSchema()\n",
        "\n",
        "df_index.show()"
      ],
      "metadata": {
        "id": "zrhP6XXHKRso"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Observamos que foram criadas novas colunas com valores numericos e cada coluna se referencia a um valor específico, por exemplo: o destinatario_banco de valor 4 tem valor valor caixa, nubank tem valor 2. Simplesmente ele criou um mapeamento entre os bancos pra um valor numerico, isso pra poder treinar o nosso modelo."
      ],
      "metadata": {
        "id": "F7nIXIWJKYHM"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "O próximo passo pra executar o algotimo de machine learn é criar dois dataframes. Um que seja so com fraudes e outro sem fraudes. Para isso a gente pode usar a função de filtro, pegando dataframe atual e filtrar os valores que forem 1, que é fraude."
      ],
      "metadata": {
        "id": "kZq7f581MUtb"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "is_fraud = df_index.filter('fraude == 1')  #filter(col('fraude') == 1)\n",
        "\n",
        "no_fraud = df_index.filter('fraude == 0')  "
      ],
      "metadata": {
        "id": "PggZEAhnPqyv"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Precisa pegar uma amostra aleatória de dentro do dataframe dos dados que não é fraude usando sample, vou pegar 1% das minhas transações e seed que é um numero aleatório, por exemplo 123 pra ele gerar uma base aleatória."
      ],
      "metadata": {
        "id": "t1QCBgSTP8Ao"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "no_fraud = no_fraud.sample(False, 0.01, seed=123)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 165
        },
        "id": "hc1PoDpHQYO4",
        "outputId": "a4c6ca7a-2d15-4738-e162-9bc4d7398145"
      },
      "execution_count": 32,
      "outputs": [
        {
          "output_type": "error",
          "ename": "NameError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-32-955c35fd2bdc>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mno_fraud\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mno_fraud\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msample\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m0.01\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mseed\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m123\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m: name 'no_fraud' is not defined"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Proxima coisa é unir o dataframe antigo de fraude com a amostra que eu fiz de não fraude.Para isso vamos usar o concat"
      ],
      "metadata": {
        "id": "hbsb4mRzQ52F"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "df_concat = no_fraud.union(is_fraud)\n",
        "\n",
        "df = df_concat.sort('transaction_date')\n",
        "\n",
        "df.count()"
      ],
      "metadata": {
        "id": "P-DQ4U36RKEI"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Proximo passo é separa uma base de treino e teste "
      ],
      "metadata": {
        "id": "2ttZDpUjRlAS"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Criando nossa base de treino e teste. O random Split ele divide o dataframe em dois dataframes separados e eu posso mandar pra ele uma lista, pro primeiro 70% vai pra treino e 30% test, e o seed um numero aleatorio"
      ],
      "metadata": {
        "id": "a98_XsjFRrV-"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "train, test = df.randomSplit([0.7,0.3], seed= 123)"
      ],
      "metadata": {
        "id": "mta0GfenRveK"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Agora podemos ver quantos dados foram pra cada um pra ver se 70% dos dados foi pra treino e 30% foi pra teste"
      ],
      "metadata": {
        "id": "MBEUjHuQSIRQ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "print('train', train.count(), 'test', test.count())"
      ],
      "metadata": {
        "id": "x1ELoC93SR7K"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Agora precisamos que na nossa base de treino não tenha erros. Logo precisamos criar uma nova coluna pra provar pro dataframe que essa linha é uma fraude essa linha não é. Pra isso vamos criar uma udf, que é basicamente digamos que vc queria executar um código python mas dentro do spark paralelizando os dados. Lembrando que sempre que se executa um código em python ele não paraleliza os dados. Somente quando se faz isso através do dataframe. "
      ],
      "metadata": {
        "id": "Q4aaTIz1SaPp"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "O que a udf faz é identificar se a linha é uma fraude ou não através de um lambda, que é uma função específica do python que intera sobre o valor que vai entrar e faz uma ação em uma linha. Ele pega o valor que vai entrar e dependendo da condição vai retornar um valor ou outro."
      ],
      "metadata": {
        "id": "rlUG9j25TUcv"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "is_fraud = udf(lambda fraud: 1.0 if fraud > 0.0, DoubleType())\n",
        "\n",
        "train = train.withColumn('is_fraud', is_fraud(train.fraude))"
      ],
      "metadata": {
        "id": "Xg4tRgrbTrrD"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "train = train.withColumn('is_fraud', is_fraud(train.fraude)) foi pra criar uma nova coluna que de fato vai ser usada no modelo"
      ],
      "metadata": {
        "id": "-iNg9VVlUSb0"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "train.printSchema()"
      ],
      "metadata": {
        "id": "lw9QH8JgULmz"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Agora vamos importar as ferramentas de pyspark, isso tem na documentação do sparkML"
      ],
      "metadata": {
        "id": "ZRNbMFO1UJE6"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Como regressão logistica é um modelo de classificação, iremos importar lá da biblioteca de classification. Agora precisamos importar o VectorAssembler e por ultimo o Pipeline"
      ],
      "metadata": {
        "id": "cTDzZ0I3UwiQ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from pyspark.ml.classification import LogisticRegression\n",
        "from pyspark.ml.feature import VectorAssembler\n",
        "from pyspark.ml import Pipeline\n"
      ],
      "metadata": {
        "id": "_Lowz0IqYEde"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Agora vamos criar cada um, primeira mente vamos criar um vectorAssembler, vai pegar todos os valores de uma linha vai unir em uma coluna só todos aqueles valores de uma lista.\n",
        "\n",
        "Então precisams criar uma variavel Assembler e vamos passsar dois parametros - inputCols, que é onde vmos pegar todas as nossas colunas de treinamento que queira unir "
      ],
      "metadata": {
        "id": "z5EH6BN_YoMS"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "assembler = VectorAssembler(\n",
        "    inputCols = [x for x in train.columns if x not in ('transaction_date', 'fraud', 'is_fraud')],\n",
        "    outputCol = 'feature'\n",
        ")"
      ],
      "metadata": {
        "id": "yNKybVVeZNi2"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "inputCols = [x for x in train.columns if x not in ('')] vai passar de coluna a coluna e vai colocar nessa lista de inputCols, porém somente as colunas que não foram a transaction_date pq não quero que elas se tornem vetor"
      ],
      "metadata": {
        "id": "LVYDNa_MZgeo"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Agora vai pegar todas as colunas que não são as 3 de cima e vou criar outra coluna que vai se chamar feature. Ou seja, vai pegar todas as coluna e transformar em um vetor de valores."
      ],
      "metadata": {
        "id": "kPTmZahXaJiu"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Foi criado os destinatários index anteriormente, porém temos os dados duplicados, então precisar dropar eles"
      ],
      "metadata": {
        "id": "ziuVEpkxap34"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "assembler = VectorAssembler(\n",
        "    inputCols = [x for x in train.columns if x not in ('transaction_date', 'fraud', 'is_fraud', 'destinatario_nome',\n",
        "                                                       'destinatario_banco', 'destinatario_tipo', 'chave_pix', 'categoria')],\n",
        "    outputCol = 'features'\n",
        ")"
      ],
      "metadata": {
        "id": "_6VjWWjba2HO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "outputCol = 'features' deve ser escrito assim e não feature, atenção!"
      ],
      "metadata": {
        "id": "sE85NmjNd5a3"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Agora precisamos criar o modelo de regrassão logistica"
      ],
      "metadata": {
        "id": "oXseR_T4bI72"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Vamos referenciala com lr e vamos instaciar o objeto que importamos\n",
        "\n",
        "setParams vai setar os paramentros da nossa regressão logistica\n",
        "\n",
        "O maxIter é o maximo de interações que podemos fazer, é facultativo\n",
        "\n",
        "labelCol que é o que vai definir se a nossa transação é fraudulosa ou não\n",
        "\n",
        "predictionCol é a coluna que vai criar com resultado da predição que ela fez\n"
      ],
      "metadata": {
        "id": "l19X2E5db9ZX"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "lr = LogisticRegression().setParams(\n",
        "    maxIter = 100000,\n",
        "    labelCol = 'is_fraud',\n",
        "    predictionCol='prediction'\n",
        ")"
      ],
      "metadata": {
        "id": "MbDWOMhnbL_5"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Note que essa funções ainda não foram feitos no dataframe, pra isso precisa primeiro executar o assembler de depois o lostic Regression pra isso vamos usar i pip line que usamos lá em cima e ele leva apenas um parametro que é o stage que é uma lista que vai colocar a ordem do que vai ser executado...por ultimo fazer um fit pra treinar o modelo, vai pegar o dataset de input e vai colocar dentro do modelo"
      ],
      "metadata": {
        "id": "SC0OmuGdcmRu"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "model = PipeLine(stages=[assembler, lr]).fit(train)"
      ],
      "metadata": {
        "id": "CmJRsI-ldMuq"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Agora vamos testar pra ver se a previsão foi correta"
      ],
      "metadata": {
        "id": "2sfOWE9Vdhao"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Podemos criar um novo dataframe, e do nosso modelo vamos usar a função transform, ou seja, vamos pegar um dataframe de entrada e verificar se existe uma fraude ou não. O primeiro dataframe vai ser o de teste que criamos lá atrás, que é um teste aleatorio"
      ],
      "metadata": {
        "id": "6zcQ4kCDeUOm"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "predicted = model.transform(test)"
      ],
      "metadata": {
        "id": "pW_lL8uaeydE"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "predicted.show()"
      ],
      "metadata": {
        "id": "EmCwapO0e90-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "O retorno desse dataframe vai ser o prorpio dataframe, porém com uma nova coluna que foi criada com nosso modelo. Nesse caso os primeiros casos foram identifcados como fraude. Agora como vamos verificar a quantidade de fraudes que foi acertada ou não."
      ],
      "metadata": {
        "id": "KZTz4oy2e_mK"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Vemos a matriz de acerto desse modelo, vamos pegar esse dataframe e vamos modificar uma coluna e aplicar o lambda que foi criado anteriormente"
      ],
      "metadata": {
        "id": "IZPBgv_1fSKB"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "predicted = predicted.withColumn('is_fraud', is_fraud(predicted.fraud))"
      ],
      "metadata": {
        "id": "EpBnPHPjfe9P"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Estamos pegando e criando uma nova coluna is_fraud que ela não existe no df, e estamos pegando o valor que setamos la no dataset que identificamos anteriormente. Esse valor predicted.fraud é o correto, agora vamos fazer um crosstab entre essa coluna predicted.fraud que é a correta e a predição pra ver se funcionou"
      ],
      "metadata": {
        "id": "Hzy0UscWf7Ua"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "predicted.crosstab('is_fraud', 'prediction').show()"
      ],
      "metadata": {
        "id": "5Ex3Cck8gaUi"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "criada a nossa matriz "
      ],
      "metadata": {
        "id": "iDxsXWINgnPD"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Agora vamos criar um exemplo nosso pra vermos como que vai a predição do algoritmo. Precisamos criar uma coluna com nome e valor"
      ],
      "metadata": {
        "id": "MzDNDCfBgxtE"
      }
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "kIG4sruRg7w-"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}